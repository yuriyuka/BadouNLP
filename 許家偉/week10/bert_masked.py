#coding:utf8

import torch
import torch.nn as nn
import numpy as np
import math
import random
import os
import re
from transformers import BertTokenizer, BertModel, BertConfig
import torch.nn.functional as F

"""
ç”¨Bert + mask æ›¿æ¢lstm å®Œæˆè‡ªå›æ­¸èªè¨€ç”Ÿæˆä»»å‹™
"""

# ç°¡åŒ–ç‰ˆæœ¬çš„BERTæ¨¡å‹ - é¿å…é…ç½®å•é¡Œ
class SimpleBertModel(nn.Module):
    def __init__(self, vocab_size, hidden_size=256, num_layers=4, num_heads=8):
        super(SimpleBertModel, self).__init__()
        
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        
        # è©åµŒå…¥å±¤
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        
        # ä½ç½®ç·¨ç¢¼
        self.position_embedding = nn.Embedding(512, hidden_size)
        
        # å¤šå±¤Transformerç·¨ç¢¼å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_size,
            nhead=num_heads,
            dim_feedforward=hidden_size * 4,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # èªè¨€æ¨¡å‹é ­éƒ¨
        self.lm_head = nn.Linear(hidden_size, vocab_size)
        
        # Dropout
        self.dropout = nn.Dropout(0.1)
        
        # æå¤±å‡½æ•¸
        self.loss = nn.functional.cross_entropy
        
        # Mask token ID
        self.mask_token_id = vocab_size - 1
    
    def forward(self, x, y=None, mask_positions=None):
        batch_size, seq_len = x.shape
        
        # å‰µå»ºä½ç½®ç·¨ç¢¼
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)
        
        # è©åµŒå…¥ + ä½ç½®ç·¨ç¢¼
        x_emb = self.embedding(x) + self.position_embedding(positions)
        x_emb = self.dropout(x_emb)
        
        # Transformerç·¨ç¢¼
        hidden_states = self.transformer(x_emb)
        
        # èªè¨€æ¨¡å‹é ­éƒ¨
        logits = self.lm_head(hidden_states)
        
        if y is not None:
            # è¨“ç·´æ¨¡å¼ï¼šè¨ˆç®—æå¤±
            if mask_positions is not None:
                # åªè¨ˆç®—maskä½ç½®çš„æå¤±
                loss = 0
                for i, pos in enumerate(mask_positions):
                    if isinstance(pos, list):
                        pos = pos[0] if pos else 0
                    if pos < seq_len:
                        # ä¿®å¾©ç¶­åº¦å•é¡Œï¼šç¢ºä¿ç›®æ¨™æ˜¯æ­£ç¢ºçš„å½¢ç‹€
                        target_token = y[i, pos].unsqueeze(0)  # [1]
                        pred_logits = logits[i, pos].unsqueeze(0)  # [1, vocab_size]
                        loss += self.loss(pred_logits, target_token)
                return loss / len(mask_positions) if len(mask_positions) > 0 else loss
            else:
                # è¨ˆç®—æ‰€æœ‰ä½ç½®çš„æå¤±
                return self.loss(logits.view(-1, self.vocab_size), y.view(-1))
        else:
            # æ¨ç†æ¨¡å¼ï¼šè¿”å›æ¦‚ç‡åˆ†å¸ƒ
            return torch.softmax(logits, dim=-1)


class LanguageModel(nn.Module):
    def __init__(self, vocab_size, hidden_size=768, num_layers=6, num_heads=12):
        super(LanguageModel, self).__init__()
        
        # ä¿å­˜é‡è¦åƒæ•¸
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        
        # ä½¿ç”¨transformersçš„BertConfig
        self.config = BertConfig(
            vocab_size=vocab_size,
            hidden_size=hidden_size,
            num_hidden_layers=num_layers,
            num_attention_heads=num_heads,
            max_position_embeddings=512,
            type_vocab_size=1,
            pad_token_id=0,
            mask_token_id=vocab_size - 1  # ä½¿ç”¨æœ€å¾Œä¸€å€‹tokenä½œç‚ºmask
        )
        
        # å‰µå»ºBERTæ¨¡å‹
        self.bert = BertModel(self.config)
        
        # å‰µå»ºèªè¨€æ¨¡å‹é ­éƒ¨ï¼ˆç”¨æ–¼é æ¸¬è¢«maskçš„tokenï¼‰
        self.lm_head = nn.Linear(hidden_size, vocab_size)
        
        # æ·»åŠ dropouté˜²æ­¢éæ“¬åˆ
        self.dropout = nn.Dropout(0.1)
        
        # å®šç¾©æå¤±å‡½æ•¸
        self.loss = nn.functional.cross_entropy

    # å‰å‘å‚³æ’­æ–¹æ³• - å¯¦ç¾BERT+Maskçš„é‚è¼¯
    def forward(self, x, y=None, mask_positions=None):
        # x: è¼¸å…¥åºåˆ— (batch_size, seq_len)
        # y: ç›®æ¨™åºåˆ— (batch_size, seq_len) 
        # mask_positions: maskä½ç½®åˆ—è¡¨
        
        batch_size, seq_len = x.shape
        
        # æ­¥é©Ÿ1: å‰µå»ºattention maskï¼ˆå‘Šè¨´BERTå“ªäº›ä½ç½®éœ€è¦é—œæ³¨ï¼‰
        attention_mask = torch.ones(batch_size, seq_len, dtype=torch.long)
        
        # æ­¥é©Ÿ2: å¦‚æœæä¾›äº†maskä½ç½®ï¼Œå‰‡åœ¨è©²ä½ç½®é€²è¡Œmask
        if mask_positions is not None:
            for i, pos in enumerate(mask_positions):
                if isinstance(pos, list):
                    # å¦‚æœposæ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€å€‹å…ƒç´ 
                    pos = pos[0] if pos else 0
                if pos < seq_len:
                    x[i, pos] = self.config.mask_token_id  # å°‡æŒ‡å®šä½ç½®æ›¿æ›ç‚ºmask token
        
        # æ­¥é©Ÿ3: BERTå‰å‘å‚³æ’­
        outputs = self.bert(
            input_ids=x,
            attention_mask=attention_mask,
            return_dict=True
        )
        
        # æ­¥é©Ÿ4: ç²å–æœ€å¾Œä¸€å±¤çš„éš±è—ç‹€æ…‹
        hidden_states = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)
        
        # æ­¥é©Ÿ5: é€šéèªè¨€æ¨¡å‹é ­éƒ¨é æ¸¬è©å½™
        logits = self.lm_head(hidden_states)  # (batch_size, seq_len, vocab_size)
        
        # æ­¥é©Ÿ6: æ ¹æ“šæ˜¯å¦æä¾›ç›®æ¨™ä¾†æ±ºå®šè¿”å›æå¤±é‚„æ˜¯é æ¸¬çµæœ
        if y is not None:
            # è¨“ç·´æ¨¡å¼ï¼šè¨ˆç®—æå¤±
            if mask_positions is not None:
                # åªè¨ˆç®—maskä½ç½®çš„æå¤± - BERTçš„æ¨™æº–åšæ³•
                loss = 0
                for i, pos in enumerate(mask_positions):
                    if pos < seq_len:
                        loss += self.loss(logits[i:i+1, pos:pos+1], y[i:i+1, pos:pos+1])
                return loss / len(mask_positions) if len(mask_positions) > 0 else loss
            else:
                # è¨ˆç®—æ‰€æœ‰ä½ç½®çš„æå¤±
                return self.loss(logits.view(-1, self.vocab_size), y.view(-1))
        else:
            # æ¨ç†æ¨¡å¼ï¼šè¿”å›æ¦‚ç‡åˆ†å¸ƒ
            return torch.softmax(logits, dim=-1)

#åŠ è½½å­—è¡¨
def build_vocab(vocab_path):
    vocab = {"<pad>":0}
    with open(vocab_path, encoding="utf8") as f:
        for index, line in enumerate(f):
            char = line[:-1]       #å»æ‰ç»“å°¾æ¢è¡Œç¬¦
            vocab[char] = index + 1 #ç•™å‡º0ä½ç»™pad token
    return vocab

#åŠ è½½è¯­æ–™
def load_corpus(path):
    corpus = ""
    with open(path, encoding="gbk") as f:
        for line in f:
            corpus += line.strip()
    return corpus

#éšæœºç”Ÿæˆä¸€ä¸ªæ ·æœ¬ - ä¿®æ”¹ä¸ºBERT Maskè¯­è¨€æ¨¡å‹
#ä»æ–‡æœ¬ä¸­æˆªå–éšæœºçª—å£ï¼Œéšæœºmaskä¸€äº›ä½ç½®è¿›è¡Œé¢„æµ‹
def build_sample(vocab, window_size, corpus, mask_ratio=0.15):
    # å¾èªæ–™ä¸­éš¨æ©Ÿé¸æ“‡ä¸€å€‹çª—å£
    start = random.randint(0, len(corpus) - 1 - window_size)
    end = start + window_size
    window = corpus[start:end]
    
    # å°‡å­—ç¬¦è½‰æ›ç‚ºæ•¸å­—ID
    x = [vocab.get(word, vocab["<UNK>"]) for word in window]
    y = x.copy()  # ç›®æ¨™åºåˆ—èˆ‡è¼¸å…¥åºåˆ—ç›¸åŒ
    
    # éš¨æ©Ÿé¸æ“‡ä¸€äº›ä½ç½®é€²è¡Œmask
    mask_positions = []
    vocab_size = len(vocab)
    
    for i in range(len(x)):
        if random.random() < mask_ratio:  # 15%çš„æ¦‚ç‡é€²è¡Œmask
            mask_positions.append(i)
            y[i] = x[i]  # ç›®æ¨™æ˜¯é æ¸¬è¢«maskçš„åŸå§‹token
            x[i] = vocab.get("<MASK>", vocab_size - 1)  # ä½¿ç”¨mask tokenæ›¿æ›
    
    return x, y, mask_positions

#å»ºç«‹æ•°æ®é›† - ä¿®æ”¹ä¸ºæ”¯æŒBERT Maskè¯­è¨€æ¨¡å‹
#sample_length è¾“å…¥éœ€è¦çš„æ ·æœ¬æ•°é‡ã€‚éœ€è¦å¤šå°‘ç”Ÿæˆå¤šå°‘
#vocab è¯è¡¨
#window_size æ ·æœ¬é•¿åº¦
#corpus è¯­æ–™å­—ç¬¦ä¸²
def build_dataset(sample_length, vocab, window_size, corpus):
    dataset_x = []
    dataset_y = []
    mask_positions_list = []
    
    # ç”ŸæˆæŒ‡å®šæ•¸é‡çš„æ¨£æœ¬
    for i in range(sample_length):
        x, y, mask_positions = build_sample(vocab, window_size, corpus)
        
        # åœ¨æ•¸æ“šæº–å‚™éšæ®µå°±æ‡‰ç”¨mask
        if mask_positions:
            for pos in mask_positions:
                if pos < len(x):
                    x[pos] = vocab.get("<MASK>", len(vocab) - 1)
        
        dataset_x.append(x)
        dataset_y.append(y)
        # ç¢ºä¿mask_positionsæ˜¯æ•´æ•¸åˆ—è¡¨
        if mask_positions:
            mask_positions_list.append(mask_positions[0] if len(mask_positions) > 0 else 0)
        else:
            mask_positions_list.append(0)
    
    # è½‰æ›ç‚ºtensorä¸¦è¿”å›
    return torch.LongTensor(dataset_x), torch.LongTensor(dataset_y), mask_positions_list

#å»ºç«‹æ¨¡å‹ - ä½¿ç”¨ç°¡åŒ–ç‰ˆæœ¬çš„BERTæ¨¡å‹
def build_model(vocab, hidden_size=256):
    # ç²å–è©å½™è¡¨å¤§å°
    vocab_size = len(vocab)
    
    # å‰µå»ºç°¡åŒ–ç‰ˆæœ¬çš„BERTèªè¨€æ¨¡å‹
    model = SimpleBertModel(vocab_size, hidden_size)
    
    return model

#æ–‡æœ¬ç”Ÿæˆæµ‹è¯•ä»£ç  - ä¿®æ”¹ä¸ºBERTç”Ÿæˆæ–¹å¼
def generate_sentence(openings, model, vocab, window_size):
    reverse_vocab = dict((y, x) for x, y in vocab.items())
    model.eval()
    
    with torch.no_grad():
        pred_char = ""
        #ç”Ÿæˆäº†æ¢è¡Œç¬¦ï¼Œæˆ–ç”Ÿæˆæ–‡æœ¬è¶…è¿‡30å­—åˆ™ç»ˆæ­¢è¿­ä»£
        while pred_char != "\n" and len(openings) <= 30:
            openings += pred_char
            
            # å–æœ€å¾Œwindow_sizeå€‹å­—ç¬¦ä½œç‚ºä¸Šä¸‹æ–‡
            current_text = openings[-window_size:]
            x = [vocab.get(char, vocab["<UNK>"]) for char in current_text]
            
            # åœ¨æœ€å¾Œä¸€å€‹ä½ç½®æ·»åŠ mask token
            x.append(vocab.get("<MASK>", len(vocab) - 1))
            x = torch.LongTensor([x])
            
            if torch.cuda.is_available():
                x = x.cuda()
            
            # é æ¸¬maskä½ç½®çš„token
            y = model(x)[0][-1]  # å–æœ€å¾Œä¸€å€‹ä½ç½®ï¼ˆmaskä½ç½®ï¼‰çš„é æ¸¬
            index = sampling_strategy(y)
            pred_char = reverse_vocab[index]
    
    return openings

def sampling_strategy(prob_distribution):
    if random.random() > 0.1:
        strategy = "greedy"
    else:
        strategy = "sampling"

    if strategy == "greedy":
        return int(torch.argmax(prob_distribution))
    elif strategy == "sampling":
        prob_distribution = prob_distribution.cpu().numpy()
        return np.random.choice(list(range(len(prob_distribution))), p=prob_distribution)


#è®¡ç®—æ–‡æœ¬ppl - ä¿®æ”¹ä¸ºBERTæ–¹å¼
def calc_perplexity(sentence, model, vocab, window_size):
    prob = 0
    model.eval()
    
    with torch.no_grad():
        for i in range(1, len(sentence)):
            # æ­¥é©Ÿ1: ç²å–ä¸Šä¸‹æ–‡çª—å£
            start = max(0, i - window_size)
            window = sentence[start:i]
            x = [vocab.get(char, vocab["<UNK>"]) for char in window]
            
            # æ­¥é©Ÿ2: åœ¨æœ€å¾Œæ·»åŠ mask token
            x.append(vocab.get("<MASK>", len(vocab) - 1))
            x = torch.LongTensor([x])
            
            # æ­¥é©Ÿ3: ç²å–ç›®æ¨™token
            target = sentence[i]
            target_index = vocab.get(target, vocab["<UNK>"])
            
            if torch.cuda.is_available():
                x = x.cuda()
            
            # æ­¥é©Ÿ4: é æ¸¬ä¸¦è¨ˆç®—æ¦‚ç‡
            pred_prob_distribute = model(x)[0][-1]  # å–maskä½ç½®çš„é æ¸¬
            target_prob = pred_prob_distribute[target_index]
            prob += math.log(target_prob, 10)
    
    return 2 ** (prob * ( -1 / len(sentence)))


def train(corpus_path, save_weight=True):
    epoch_num = 10        #è®­ç»ƒè½®æ•°
    batch_size = 64       #æ¯æ¬¡è®­ç»ƒæ ·æœ¬ä¸ªæ•°
    train_sample = 50000   #æ¯è½®è®­ç»ƒæ€»å…±è®­ç»ƒçš„æ ·æœ¬æ€»æ•°
    hidden_size = 256     #ç°¡åŒ–BERTéš±è—å±¤å¤§å°
    window_size = 10       #æ ·æœ¬æ–‡æœ¬é•¿åº¦
    vocab = build_vocab("vocab.txt")       #å»ºç«‹å­—è¡¨
    corpus = load_corpus(corpus_path)     #åŠ è½½è¯­æ–™
    
    # å‰µå»ºç°¡åŒ–BERTæ¨¡å‹
    model = build_model(vocab, hidden_size)    #å»ºç«‹æ¨¡å‹
    if torch.cuda.is_available():
        model = model.cuda()
    
    # è¨­ç½®å„ªåŒ–å™¨ï¼ˆBERTéœ€è¦è¼ƒä½çš„å­¸ç¿’ç‡ï¼‰
    optim = torch.optim.Adam(model.parameters(), lr=0.0001)   #å»ºç«‹ä¼˜åŒ–å™¨ï¼Œé™ä½å­¸ç¿’ç‡
    
    print("ç°¡åŒ–BERT Maskèªè¨€æ¨¡å‹åŠ è¼‰å®Œç•¢ï¼Œé–‹å§‹è¨“ç·´")
    
    # æ­¥é©Ÿ5: é–‹å§‹è¨“ç·´å¾ªç’°
    for epoch in range(epoch_num):
        model.train()
        watch_loss = []
        
        for batch in range(int(train_sample / batch_size)):
            # æ­¥é©Ÿ6: æ§‹å»ºè¨“ç·´æ¨£æœ¬
            x, y, mask_positions = build_dataset(batch_size, vocab, window_size, corpus)
            
            if torch.cuda.is_available():
                x, y = x.cuda(), y.cuda()
                # mask_positionsæ˜¯åˆ—è¡¨ï¼Œä¸éœ€è¦è½‰ç§»åˆ°GPU
            
            # æ­¥é©Ÿ7: å‰å‘å‚³æ’­å’Œåå‘å‚³æ’­
            optim.zero_grad()    #æ¢¯åº¦å½’é›¶
            loss = model(x, y, mask_positions)   #è®¡ç®—loss
            loss.backward()      #è®¡ç®—æ¢¯åº¦
            optim.step()         #æ›´æ–°æƒé‡
            watch_loss.append(loss.item())
        print("=========\nç¬¬%dè½®å¹³å‡loss:%f" % (epoch + 1, np.mean(watch_loss)))
        print(generate_sentence("è®©ä»–åœ¨åŠå¹´ä¹‹å‰ï¼Œå°±ä¸èƒ½åšå‡º", model, vocab, window_size))
        print(generate_sentence("ææ…•ç«™åœ¨å±±è·¯ä¸Šï¼Œæ·±æ·±çš„å‘¼å¸", model, vocab, window_size))
    if not save_weight:
        return
    else:
        base_name = os.path.basename(corpus_path).replace("txt", "pth")
        model_path = os.path.join("model", base_name)
        torch.save(model.state_dict(), model_path)
        return

# æ¸¬è©¦å‡½æ•¸ - é©—è­‰ç°¡åŒ–BERTæ¨¡å‹çš„åŸºæœ¬åŠŸèƒ½
def test_bert_model():
    """
    æ¸¬è©¦ç°¡åŒ–BERTæ¨¡å‹çš„åŸºæœ¬åŠŸèƒ½
    """
    print("é–‹å§‹æ¸¬è©¦ç°¡åŒ–BERTæ¨¡å‹...")
    
    # å‰µå»ºä¸€å€‹ç°¡å–®çš„è©å½™è¡¨
    test_vocab = {"<pad>": 0, "<UNK>": 1, "<MASK>": 2, "a": 3, "b": 4, "c": 5}
    
    # å‰µå»ºä¸€å€‹å°å‹çš„ç°¡åŒ–BERTæ¨¡å‹
    model = SimpleBertModel(len(test_vocab), hidden_size=64, num_layers=2, num_heads=4)
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    test_input = torch.LongTensor([[3, 4, 5, 2]])  # "abc[MASK]"
    test_target = torch.LongTensor([[3, 4, 5, 3]])  # ç›®æ¨™æ˜¯é æ¸¬"a"
    
    # æ¸¬è©¦å‰å‘å‚³æ’­
    try:
        output = model(test_input)
        print(f"âœ… å‰å‘å‚³æ’­æˆåŠŸï¼Œè¼¸å‡ºå½¢ç‹€: {output.shape}")
        
        # æ¸¬è©¦æå¤±è¨ˆç®—
        loss = model(test_input, test_target, mask_positions=[3])
        print(f"âœ… æå¤±è¨ˆç®—æˆåŠŸï¼Œæå¤±å€¼: {loss.item():.4f}")
        
        print("ğŸ‰ ç°¡åŒ–BERTæ¨¡å‹æ¸¬è©¦é€šéï¼")
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # å…ˆé‹è¡Œæ¸¬è©¦ç¢ºä¿æ¨¡å‹æ­£å¸¸å·¥ä½œ
    #test_bert_model()
    
    # å¦‚æœæ¸¬è©¦é€šéï¼Œå†é‹è¡Œè¨“ç·´
    print("\né–‹å§‹è¨“ç·´æ¨¡å‹...")
    # build_vocab_from_corpus("corpus/all.txt")
    train("corpus.txt", True)