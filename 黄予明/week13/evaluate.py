# -*- coding: utf-8 -*-

import torch
import numpy as np
from collections import defaultdict
from sklearn.metrics import classification_report
from loader import load_data

class Evaluator:
    def __init__(self, config, model, logger):
        self.config = config
        self.model = model  
        self.logger = logger
        self.valid_data = load_data(config["valid_data_path"], config, shuffle=False)
        
        # è·å–æ ‡ç­¾æ˜ å°„
        self.index_to_label = self.valid_data.dataset.index_to_label
        self.label_to_index = self.valid_data.dataset.label_to_index
        
        # ç¡®å®šè®¾å¤‡
        if torch.cuda.is_available():
            self.device = torch.device("cuda")
        elif torch.backends.mps.is_available():
            self.device = torch.device("mps")
        else:
            self.device = torch.device("cpu")

    def extract_entities(self, tokens, labels, index_to_label):
        """ä»BIOæ ‡ç­¾åºåˆ—ä¸­æå–å®ä½“"""
        entities = []
        current_entity = None
        
        for i, (token, label_id) in enumerate(zip(tokens, labels)):
            if label_id == -100:  # è·³è¿‡å¡«å……token
                continue
                
            label = index_to_label.get(label_id, 'O')
            
            if label == 'O':
                if current_entity:
                    entities.append(current_entity)
                    current_entity = None
            elif label.startswith('B-'):
                if current_entity:
                    entities.append(current_entity)
                entity_type = label[2:]  # å»æ‰'B-'å‰ç¼€
                current_entity = {
                    'type': entity_type,
                    'start': i,
                    'end': i,
                    'text': str(token) if hasattr(token, '__str__') else ''
                }
            elif label.startswith('I-'):
                if current_entity and label[2:] == current_entity['type']:
                    current_entity['end'] = i
                    current_entity['text'] += str(token) if hasattr(token, '__str__') else ''
                else:
                    # ä¸åŒ¹é…çš„Iæ ‡ç­¾ï¼Œå½“ä½œæ–°å®ä½“å¼€å§‹
                    if current_entity:
                        entities.append(current_entity)
                    entity_type = label[2:]
                    current_entity = {
                        'type': entity_type,
                        'start': i,
                        'end': i,
                        'text': str(token) if hasattr(token, '__str__') else ''
                    }
        
        if current_entity:
            entities.append(current_entity)
        
        return entities

    def calculate_entity_metrics(self, true_entities, pred_entities):
        """è®¡ç®—å®ä½“çº§åˆ«çš„precision, recall, F1"""
        # è½¬æ¢ä¸ºé›†åˆï¼Œç”¨äºè®¡ç®—äº¤é›†
        true_set = {(e['type'], e['start'], e['end']) for e in true_entities}
        pred_set = {(e['type'], e['start'], e['end']) for e in pred_entities}
        
        tp = len(true_set & pred_set)
        fp = len(pred_set - true_set)
        fn = len(true_set - pred_set)
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        
        return precision, recall, f1, tp, fp, fn

    def calculate_entity_metrics_by_type(self, true_entities, pred_entities):
        """æŒ‰å®ä½“ç±»å‹è®¡ç®—è¯¦ç»†æŒ‡æ ‡"""
        # æŒ‰ç±»å‹åˆ†ç»„
        true_by_type = defaultdict(list)
        pred_by_type = defaultdict(list)
        
        for entity in true_entities:
            true_by_type[entity['type']].append((entity['start'], entity['end']))
        
        for entity in pred_entities:
            pred_by_type[entity['type']].append((entity['start'], entity['end']))
        
        # æ”¶é›†æ‰€æœ‰å®ä½“ç±»å‹
        all_types = set(true_by_type.keys()) | set(pred_by_type.keys())
        
        type_metrics = {}
        for entity_type in all_types:
            true_set = set(true_by_type[entity_type])
            pred_set = set(pred_by_type[entity_type])
            
            tp = len(true_set & pred_set)
            fp = len(pred_set - true_set)
            fn = len(true_set - pred_set)
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            
            type_metrics[entity_type] = {
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'tp': tp,
                'fp': fp,
                'fn': fn,
                'support': len(true_set)
            }
        
        return type_metrics

    def eval(self, epoch):
        self.logger.info("å¼€å§‹æµ‹è¯•ç¬¬%dè½®æ¨¡å‹æ•ˆæœï¼š" % epoch)
        self.model.eval()
        
        all_predictions = []
        all_labels = []
        all_true_entities = []
        all_pred_entities = []
        
        with torch.no_grad():
            for index, batch_data in enumerate(self.valid_data):
                batch_data = [d.to(self.device) for d in batch_data]
                input_ids, labels = batch_data
                
                # åˆ›å»ºattention_mask
                attention_mask = (input_ids != 0).float()
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                pred_logits = outputs.logits
                predictions = torch.argmax(pred_logits, dim=-1)
                
                # å¤„ç†æ¯ä¸ªæ ·æœ¬
                for i in range(input_ids.size(0)):
                    # è·å–æœ‰æ•ˆä½ç½®ï¼ˆéå¡«å……ï¼‰
                    valid_positions = (labels[i] != -100)
                    if not valid_positions.any():
                        continue
                    
                    # æå–æœ‰æ•ˆçš„é¢„æµ‹å’Œæ ‡ç­¾
                    valid_preds = predictions[i][valid_positions].cpu().numpy()
                    valid_labels = labels[i][valid_positions].cpu().numpy()
                    valid_tokens = range(len(valid_preds))  # ç®€åŒ–çš„tokenè¡¨ç¤º
                    
                    all_predictions.extend(valid_preds)
                    all_labels.extend(valid_labels)
                    
                    # æå–å®ä½“
                    true_entities = self.extract_entities(valid_tokens, valid_labels, self.index_to_label)
                    pred_entities = self.extract_entities(valid_tokens, valid_preds, self.index_to_label)
                    
                    all_true_entities.extend(true_entities)
                    all_pred_entities.extend(pred_entities)
        
        # è®¡ç®—tokençº§åˆ«å‡†ç¡®ç‡
        token_accuracy = sum([p == l for p, l in zip(all_predictions, all_labels)]) / len(all_predictions) if all_predictions else 0.0
        
        # è®¡ç®—æ•´ä½“å®ä½“çº§åˆ«æŒ‡æ ‡
        entity_precision, entity_recall, entity_f1, total_tp, total_fp, total_fn = self.calculate_entity_metrics(all_true_entities, all_pred_entities)
        
        # è®¡ç®—æ¯ä¸ªå®ä½“ç±»å‹çš„è¯¦ç»†æŒ‡æ ‡
        type_metrics = self.calculate_entity_metrics_by_type(all_true_entities, all_pred_entities)
        
        # è¾“å‡ºç»“æœ
        self.logger.info("="*60)
        self.logger.info(f"ğŸ“Š ç¬¬{epoch}è½®éªŒè¯ç»“æœ")
        self.logger.info("="*60)
        
        # Tokençº§åˆ«æŒ‡æ ‡
        self.logger.info(f"ğŸ·ï¸  Tokençº§åˆ«å‡†ç¡®ç‡: {token_accuracy:.4f}")
        
        # æ•´ä½“å®ä½“çº§åˆ«æŒ‡æ ‡
        self.logger.info(f"ğŸ¯ å®ä½“çº§åˆ«æŒ‡æ ‡:")
        self.logger.info(f"   ç²¾ç¡®ç‡(Precision): {entity_precision:.4f}")
        self.logger.info(f"   å¬å›ç‡(Recall): {entity_recall:.4f}")
        self.logger.info(f"   F1åˆ†æ•°: {entity_f1:.4f}")
        self.logger.info(f"   TP={total_tp}, FP={total_fp}, FN={total_fn}")
        
        # å„å®ä½“ç±»å‹è¯¦ç»†æŒ‡æ ‡
        self.logger.info(f"\nğŸ“ˆ å„å®ä½“ç±»å‹è¡¨ç°:")
        self.logger.info(f"{'å®ä½“ç±»å‹':<12} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8} {'æ”¯æŒæ•°':<6} {'TP':<4} {'FP':<4} {'FN':<4}")
        self.logger.info("-" * 70)
        
        # æŒ‰F1åˆ†æ•°é™åºæ’åˆ—
        sorted_types = sorted(type_metrics.items(), key=lambda x: x[1]['f1'], reverse=True)
        
        for entity_type, metrics in sorted_types:
            self.logger.info(
                f"{entity_type:<12} "
                f"{metrics['precision']:<8.3f} "
                f"{metrics['recall']:<8.3f} "
                f"{metrics['f1']:<8.3f} "
                f"{metrics['support']:<6d} "
                f"{metrics['tp']:<4d} "
                f"{metrics['fp']:<4d} "
                f"{metrics['fn']:<4d}"
            )
        
        # è®¡ç®—å®å¹³å‡å’Œå¾®å¹³å‡
        if type_metrics:
            macro_precision = sum([m['precision'] for m in type_metrics.values()]) / len(type_metrics)
            macro_recall = sum([m['recall'] for m in type_metrics.values()]) / len(type_metrics)
            macro_f1 = sum([m['f1'] for m in type_metrics.values()]) / len(type_metrics)
            
            self.logger.info("-" * 70)
            self.logger.info(f"{'å®å¹³å‡':<12} {macro_precision:<8.3f} {macro_recall:<8.3f} {macro_f1:<8.3f}")
            self.logger.info(f"{'å¾®å¹³å‡':<12} {entity_precision:<8.3f} {entity_recall:<8.3f} {entity_f1:<8.3f}")
        
        # å®ä½“ç»Ÿè®¡
        total_true_entities = len(all_true_entities)
        total_pred_entities = len(all_pred_entities)
        self.logger.info(f"\nğŸ“Š å®ä½“ç»Ÿè®¡:")
        self.logger.info(f"   çœŸå®å®ä½“æ€»æ•°: {total_true_entities}")
        self.logger.info(f"   é¢„æµ‹å®ä½“æ€»æ•°: {total_pred_entities}")
        self.logger.info(f"   æ­£ç¡®è¯†åˆ«å®ä½“: {total_tp}")
        
        self.logger.info("="*60)
        
        return entity_f1  # è¿”å›å®ä½“F1ä½œä¸ºä¸»è¦è¯„ä¼°æŒ‡æ ‡

    def write_stats(self, labels, pred_results):
        # ä¿ç•™åŸæœ‰æ¥å£ï¼Œä½†ç°åœ¨åœ¨evalæ–¹æ³•ä¸­ç›´æ¥å¤„ç†
        pass

    def show_stats(self):
        # ä¿ç•™åŸæœ‰æ¥å£ï¼Œä½†ç°åœ¨åœ¨evalæ–¹æ³•ä¸­ç›´æ¥å¤„ç†
        return 0.0
