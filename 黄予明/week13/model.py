import torch.nn as nn
import os
from config import Config
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification, AutoModel
from torch.optim import Adam, SGD

def TorchModel():
    """åŠ è½½BERTæ¨¡å‹ç”¨äºNERä»»åŠ¡ï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶"""
    model_path = Config["pretrain_model_path"]
    num_labels = Config.get("class_num", 9)  # BIOæ ¼å¼æ ‡ç­¾æ•°é‡
    
    print(f"âœ… ä½¿ç”¨æœ¬åœ°BERTæ¨¡å‹: {model_path}")
    print(f"ğŸ·ï¸  NERæ ‡ç­¾æ•°é‡: {num_labels}")
    
    # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œä¸ä»ç½‘ç»œä¸‹è½½
    model = AutoModelForTokenClassification.from_pretrained(
        model_path,
        num_labels=num_labels,
        local_files_only=True,  # å¼ºåˆ¶åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        trust_remote_code=True,
        return_dict=True       # ç¡®ä¿è¿”å›å­—å…¸æ ¼å¼
    )
    
    print(f"ğŸ‰ NERæ¨¡å‹åŠ è½½æˆåŠŸï¼å‚æ•°é‡: {model.num_parameters():,}")
    return model


def choose_optimizer(config, model):
    optimizer = config["optimizer"]
    learning_rate = config["learning_rate"]
    if optimizer == "adam":
        return Adam(model.parameters(), lr=learning_rate)
    elif optimizer == "sgd":
        return SGD(model.parameters(), lr=learning_rate)
