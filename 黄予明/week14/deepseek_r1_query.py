#!/usr/bin/env python3
"""
DeepSeek R1 Model Query Script

This script allows you to interact with DeepSeek R1 models in GGUF format only.
Supported: DeepSeek-R1-Distill-Qwen-32B (GGUF)
"""

import os
import sys
import argparse
from typing import Optional, Dict, Any
import json
import subprocess

def check_dependencies(model_type: str):
    """Check if required dependencies for the selected model are installed."""
    missing_deps = []
    
    if model_type == "gguf":
        try:
            import llama_cpp  # noqa: F401
        except ImportError:
            missing_deps.append("llama-cpp-python")
    
    if missing_deps:
        print("Missing dependencies for model type '" + model_type + "'. Please install:")
        for dep in missing_deps:
            print(f"  pip install {dep}")
        if model_type == "gguf" and sys.platform == "darwin":
            print("  # On Apple Silicon for GPU acceleration:")
            print("  CMAKE_ARGS='-DLLAMA_METAL=on' pip install llama-cpp-python")
        return False
    return True

class GGUFModel:
    """Wrapper for GGUF model using llama-cpp-python."""
    
    def __init__(self, model_path: str):
        try:
            from llama_cpp import Llama
            self.model = Llama(
                model_path=model_path,
                n_ctx=4096,
                n_threads=os.cpu_count(),
                n_gpu_layers=-1,  # Use all available GPU layers
                verbose=False
            )
            print(f"âœ… Loaded GGUF model: {os.path.basename(model_path)}")
        except Exception as e:
            raise Exception(f"Failed to load GGUF model: {e}")
    
    def generate(self, prompt: str, max_tokens: int = 512, temperature: float = 0.7) -> str:
        """Generate response using GGUF model."""
        try:
            response = self.model(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                stop=["<|endoftext|>", "<|im_end|>"],
                echo=False
            )
            return response['choices'][0]['text'].strip()
        except Exception as e:
            return f"Error generating response: {e}"

def find_models():
    """Find available models in the workspace."""
    models = {}
    
    # Check for GGUF model
    gguf_path = "/Users/evan/DeepSeek-R1-Distill-Qwen-32B/bartowski/DeepSeek-R1-Distill-Qwen-32B-GGUF/DeepSeek-R1-Distill-Qwen-32B-Q4_K_S.gguf"
    if os.path.exists(gguf_path):
        models["gguf"] = {
            "name": "DeepSeek-R1-Distill-Qwen-32B (GGUF)",
            "path": gguf_path,
            "type": "gguf"
        }
    
    # MLX support removed
    
    return models

def call_rag_system(query: str) -> Dict[str, Any]:
    """è°ƒç”¨å¤–éƒ¨RAGç³»ç»Ÿè·å–ç›¸å…³æ–‡æ¡£"""
    rag_script_path = "/Users/evan/Downloads/AINLP/week14 å¤§è¯­è¨€æ¨¡å‹ç›¸å…³ç¬¬å››è®²/RAG/dota2è‹±é›„ä»‹ç»-byRAG/vec_db_rag_local.py"
    
    if not os.path.exists(rag_script_path):
        print(f"âš ï¸  RAGè„šæœ¬ä¸å­˜åœ¨: {rag_script_path}")
        return {"context": "", "retrieved_docs": [], "error": "RAG script not found"}
    
    try:
        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶Pythonè„šæœ¬æ¥è°ƒç”¨RAGç³»ç»Ÿ
        temp_script = f"""
import sys
sys.path.append('/Users/evan/Downloads/AINLP/week14 å¤§è¯­è¨€æ¨¡å‹ç›¸å…³ç¬¬å››è®²/RAG/dota2è‹±é›„ä»‹ç»-byRAG')

# å¯¼å…¥RAGæ¨¡å—
import chromadb
from chromadb.config import Settings
from local_embedding import LocalEmbeddingFunction

# åˆå§‹åŒ–ChromaDB
client = chromadb.Client(Settings(
    persist_directory="./chroma_db_local",
    is_persistent=True
))

# è·å–é›†åˆ
collection = client.get_collection("rag_local")

# æ£€ç´¢æ–‡æ¡£
results = collection.query(
    query_texts=["{query}"],
    n_results=3
)

# è¿”å›ç»“æœ
retrieved_docs = results["documents"][0] if results["documents"] else []
context = "\\n".join(retrieved_docs) if retrieved_docs else ""

print("RAG_RESULT_START")
print(context)
print("RAG_RESULT_END")
"""
        
        # æ‰§è¡Œä¸´æ—¶è„šæœ¬
        result = subprocess.run(
            [sys.executable, "-c", temp_script],
            capture_output=True,
            text=True,
            cwd="/Users/evan/Downloads/AINLP/week14 å¤§è¯­è¨€æ¨¡å‹ç›¸å…³ç¬¬å››è®²/RAG/dota2è‹±é›„ä»‹ç»-byRAG"
        )
        
        if result.returncode == 0:
            # è§£æè¾“å‡º
            output = result.stdout
            if "RAG_RESULT_START" in output and "RAG_RESULT_END" in output:
                start_idx = output.find("RAG_RESULT_START") + len("RAG_RESULT_START")
                end_idx = output.find("RAG_RESULT_END")
                context = output[start_idx:end_idx].strip()
                
                return {
                    "context": context,
                    "retrieved_docs": context.split("\n") if context else [],
                    "success": True
                }
        
        print(f"âš ï¸  RAGè°ƒç”¨å¤±è´¥: {result.stderr}")
        return {"context": "", "retrieved_docs": [], "error": result.stderr}
        
    except Exception as e:
        print(f"âš ï¸  RAGè°ƒç”¨å¼‚å¸¸: {e}")
        return {"context": "", "retrieved_docs": [], "error": str(e)}

def create_rag_prompt(query: str, rag_result: Dict[str, Any]) -> str:
    """åˆ›å»ºåŒ…å«RAGä¸Šä¸‹æ–‡çš„æç¤º"""
    context = rag_result.get("context", "")
    
    if not context:
        return query
    
    # æ„å»ºRAGå¢å¼ºçš„æç¤º
    rag_prompt = f"""åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯æ¥å›ç­”é—®é¢˜ï¼š

æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·åŸºäºä¸Šè¿°æ£€ç´¢åˆ°çš„ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœæ£€ç´¢åˆ°çš„ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜å¹¶å°½å¯èƒ½æä¾›æœ‰ç”¨çš„å›ç­”ã€‚"""
    
    return rag_prompt

def interactive_mode(model, use_rag=False):
    """Run interactive chat mode."""
    print("\nğŸ¤– Interactive mode started. Type 'quit' or 'exit' to end the session.")
    if use_rag:
        print("ğŸ” RAG mode enabled - queries will be enhanced with retrieved context")
    print("Type your questions or prompts below:\n")
    
    while True:
        try:
            user_input = input("ğŸ‘¤ You: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ Goodbye!")
                break
            
            if not user_input:
                continue
            
            # å¦‚æœå¯ç”¨RAGï¼Œå…ˆè°ƒç”¨RAGç³»ç»Ÿ
            if use_rag:
                print("ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
                rag_result = call_rag_system(user_input)
                
                if rag_result.get("success"):
                    print(f"ğŸ“„ æ£€ç´¢åˆ° {len(rag_result.get('retrieved_docs', []))} ä¸ªç›¸å…³æ–‡æ¡£")
                    enhanced_prompt = create_rag_prompt(user_input, rag_result)
                else:
                    print("âš ï¸  RAGæ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
                    enhanced_prompt = user_input
            else:
                enhanced_prompt = user_input
            
            print("ğŸ¤– DeepSeek: ", end="", flush=True)
            response = model.generate(enhanced_prompt)
            print(response)
            print()
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Goodbye!")
            break
        except Exception as e:
            print(f"âŒ Error: {e}")

sys.path.append('/Users/evan/Downloads/AINLP/week14 å¤§è¯­è¨€æ¨¡å‹ç›¸å…³ç¬¬å››è®²/RAG/dota2è‹±é›„ä»‹ç»-byRAG')

from vec_db_rag_local import get_rag_prompt

def build_prompt_for_model(user_query: str) -> str:
    # è¿™é‡Œä¼šè§¦å‘ RAG æ£€ç´¢ + æ±‡æ€»ï¼Œå¹¶è¿”å›å¯ç›´æ¥å–‚ç»™æ¨¡å‹çš„æç¤ºè¯
    return get_rag_prompt(user_query, top_k=3)

def main():
    parser = argparse.ArgumentParser(description="Query DeepSeek R1 models")
    parser.add_argument("--model", choices=["gguf"], help="Model type to use")
    parser.add_argument("--prompt", "-p", help="Single prompt to process")
    parser.add_argument("--max-tokens", "-m", type=int, default=512, help="Maximum tokens to generate")
    parser.add_argument("--temperature", "-t", type=float, default=0.7, help="Sampling temperature")
    parser.add_argument("--interactive", "-i", action="store_true", help="Run in interactive mode")
    parser.add_argument("--rag", action="store_true", help="Enable RAG mode to enhance queries with retrieved context")
    
    args = parser.parse_args()
    
    # Find available models
    models = find_models()
    
    if not models:
        print("âŒ No models found in the workspace!")
        print("Make sure you have the model files in the expected locations:")
        print("  - GGUF: bartowski/DeepSeek-R1-Distill-Qwen-32B-GGUF/")
        sys.exit(1)
    
    # Select model
    model_type = args.model
    if not model_type:
        if len(models) == 1:
            model_type = list(models.keys())[0]
        else:
            print("Available models:")
            for key, model_info in models.items():
                print(f"  {key}: {model_info['name']}")
            model_type = input("Select model type (gguf): ").strip().lower()
    
    if model_type not in models:
        print(f"âŒ Invalid model type: {model_type}")
        sys.exit(1)
    
    model_info = models[model_type]
    print(f"ğŸ“¦ Loading {model_info['name']}...")
    
    # Load model
    try:
        if not check_dependencies(model_type):
            sys.exit(1)
        model = GGUFModel(model_info["path"])
    except Exception as e:
        print(f"âŒ Failed to load model: {e}")
        sys.exit(1)
    
    # Process input
    if args.interactive:
        interactive_mode(model, use_rag=args.rag)
    elif args.prompt:
        if args.rag:
            print("ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
            rag_result = call_rag_system(args.prompt)
            if rag_result.get("success"):
                print(f"ğŸ“„ æ£€ç´¢åˆ° {len(rag_result.get('retrieved_docs', []))} ä¸ªç›¸å…³æ–‡æ¡£")
                enhanced_prompt = create_rag_prompt(args.prompt, rag_result)
            else:
                print("âš ï¸  RAGæ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
                enhanced_prompt = args.prompt
        else:
            enhanced_prompt = args.prompt
        
        print(f"ğŸ¤– Response: {model.generate(enhanced_prompt, args.max_tokens, args.temperature)}")
    else:
        # Default to interactive mode
        interactive_mode(model, use_rag=args.rag)

if __name__ == "__main__":
    main()
