# RAGç³»ç»Ÿ - ä½¿ç”¨æœ¬åœ°Embeddingæ¨¡å‹
# æ— éœ€API Keyï¼Œå®Œå…¨ç¦»çº¿è¿è¡Œ

import os
import numpy as np
import chromadb
from chromadb.config import Settings
from local_embedding import LocalEmbeddingFunction, query_to_vector_local
from bm25 import BM25
import re

print("ğŸ  å¯åŠ¨æœ¬åœ°RAGç³»ç»Ÿ...")
print("âœ… æ— éœ€API Keyï¼Œå®Œå…¨ç¦»çº¿è¿è¡Œ!")

# åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯
client = chromadb.Client(Settings(
    persist_directory="./chroma_db_local",  # æœ¬åœ°å­˜å‚¨ç›®å½•
    is_persistent=True
))

# æ¸…ç†æ—§æ•°æ®ï¼ˆå¯é€‰ï¼‰
collections = client.list_collections()
for collection in collections:
    client.delete_collection(name=collection.name)
    print(f"å·²åˆ é™¤æ—§é›†åˆ: {collection.name}")

# åˆ›å»ºä½¿ç”¨æœ¬åœ°Embeddingçš„é›†åˆ
collection_name = "rag_local"
print("ğŸ”„ æ­£åœ¨åˆ›å»ºé›†åˆå¹¶åŠ è½½æœ¬åœ°embeddingæ¨¡å‹...")

# é€‰æ‹©embeddingæ¨¡å‹ï¼ˆæ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
model_configs = {
    "lightweight": "all-MiniLM-L6-v2",          # è½»é‡çº§ï¼Œ23MB
    "multilingual": "paraphrase-multilingual-MiniLM-L12-v2"  # å¤šè¯­è¨€ï¼Œ266MB
}

# ä½¿ç”¨è½»é‡çº§æ¨¡å‹ï¼ˆæ¨èï¼‰
embedding_model = model_configs["multilingual"]

collection = client.create_collection(
    name=collection_name,
    embedding_function=LocalEmbeddingFunction(model_name=embedding_model)
)
print(f"âœ… å·²åˆ›å»ºæœ¬åœ°embeddingé›†åˆ: {collection_name}")

# ä»Heroesæ–‡ä»¶å¤¹åŠ è½½è‹±é›„æ–‡æ¡£
def load_hero_documents():
    """åŠ è½½Heroesæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰è‹±é›„æ–‡æ¡£"""
    heroes_dir = "./Heroes"
    documents = []
    ids = []
    
    if not os.path.exists(heroes_dir):
        print(f"âŒ Heroesæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {heroes_dir}")
        return [], []
    
    # è·å–æ‰€æœ‰txtæ–‡ä»¶
    txt_files = [f for f in os.listdir(heroes_dir) if f.endswith('.txt')]
    
    if not txt_files:
        print(f"âŒ åœ¨{heroes_dir}ä¸­æ²¡æœ‰æ‰¾åˆ°txtæ–‡ä»¶")
        return [], []
    
    print(f"ğŸ“ æ­£åœ¨åŠ è½½ {len(txt_files)} ä¸ªè‹±é›„æ–‡æ¡£...")
    
    for i, filename in enumerate(txt_files):
        file_path = os.path.join(heroes_dir, filename)
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if content:  # ç¡®ä¿æ–‡ä»¶ä¸ä¸ºç©º
                    documents.append(content)
                    # ä½¿ç”¨æ–‡ä»¶åï¼ˆå»æ‰.txtåç¼€ï¼‰ä½œä¸ºID
                    hero_name = filename.replace('.txt', '')
                    ids.append(f"hero_{hero_name}")
                    print(f"   âœ… å·²åŠ è½½: {hero_name}")
        except Exception as e:
            print(f"   âŒ åŠ è½½å¤±è´¥ {filename}: {e}")
    
    print(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(documents)} ä¸ªè‹±é›„æ–‡æ¡£")
    return documents, ids

# åŠ è½½è‹±é›„æ–‡æ¡£
documents, ids = load_hero_documents()

if not documents:
    print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ–‡æ¡£ï¼Œç¨‹åºé€€å‡º")
    exit(1)

print("ğŸ“ æ­£åœ¨æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“...")
print("   ç¬¬ä¸€æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´æ¥å¤„ç†å‘é‡...")

try:
    collection.add(
        documents=documents,
        ids=ids
    )
    print(f"âœ… æˆåŠŸæ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£")
except Exception as e:
    print(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
    exit(1)

# -----------------------------
# BM25 ç´¢å¼•ä¸æ··åˆæ£€ç´¢
# -----------------------------

def tokenize(text: str):
    """æç®€åˆ†è¯ï¼š
    - æå–ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ç‰‡æ®µ
    - ä¸­æ–‡æŒ‰å•å­—ä½œä¸º tokenï¼ˆæ— ç¬¬ä¸‰æ–¹ä¾èµ–ï¼Œå…¼å®¹æ€§æœ€å¥½ï¼‰
    """
    tokens = []
    # è‹±æ–‡/æ•°å­—è¯
    tokens.extend(re.findall(r"[A-Za-z0-9]+", text.lower()))
    # ä¸­æ–‡å•å­—
    chinese_spans = re.findall(r"[\u4e00-\u9fff]+", text)
    for span in chinese_spans:
        tokens.extend(list(span))
    return tokens

# æ„å»º BM25 è¯­æ–™ï¼ˆä½¿ç”¨ä¸ Chroma ç›¸åŒçš„ ids å¯¹åº”ï¼‰
id_to_doc = {ids[i]: documents[i] for i in range(len(ids))}
bm25_corpus = {ids[i]: tokenize(documents[i]) for i in range(len(documents))}
bm25_index = BM25(bm25_corpus)
print(f"ğŸ”§ å·²æ„å»º BM25 ç´¢å¼•ï¼Œæ–‡æ¡£æ•°: {len(bm25_corpus)}ï¼Œå¹³å‡é•¿åº¦: {bm25_index.avgdl:.1f}")

def search_bm25(query: str, top_k: int = 20):
    query_tokens = tokenize(query)
    scores = bm25_index.get_scores(query_tokens)
    # scores: List[[doc_id, score]]ï¼Œæ¥è‡ªå®ç°
    scores_sorted = sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]
    return scores_sorted  # [(doc_id, score)]

def search_vector(query: str, top_k: int = 20):
    try:
        results = collection.query(
            query_texts=[query],
            n_results=top_k,
            include=["documents", "ids", "distances"]
        )
        # ç»Ÿä¸€è¿”å› [(doc_id, score)]ï¼Œå°†è·ç¦»è½¬ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
        vec_ids = results.get("ids", [[]])[0]
        vec_docs = results.get("documents", [[]])[0]
        vec_dists = results.get("distances", [[]])[0]
        pairs = []
        for did, dist in zip(vec_ids, vec_dists):
            # å°†è¾ƒå°çš„è·ç¦»è½¬ä¸ºè¾ƒå¤§çš„åˆ†æ•°ï¼›æ·»åŠ ä¸€ä¸ªç¨³å®šé¡¹é˜² 0
            score = 1.0 / (1e-6 + dist)
            pairs.append((did, score))
        return pairs
    except Exception as e:
        print(f"æ£€ç´¢å¤±è´¥(Vector): {e}")
        return []

def rrf_fusion(bm25_list, vec_list, k: int = 60):
    """RRF èåˆï¼š
    è¾“å…¥ï¼š
      - bm25_list: [(doc_id, score)]ï¼ŒæŒ‰åˆ†æ•°é™åº
      - vec_list: [(doc_id, score)]ï¼ŒæŒ‰åˆ†æ•°é™åº
    è¿”å›ï¼šdoc_id -> èåˆåˆ†æ•°
    """
    rank_map = {}
    # å¯¹ BM25 æ’å
    for rank, (doc_id, _score) in enumerate(bm25_list, start=1):
        rank_map.setdefault(doc_id, 0.0)
        rank_map[doc_id] += 1.0 / (k + rank)
    # å¯¹å‘é‡ æ’å
    for rank, (doc_id, _score) in enumerate(vec_list, start=1):
        rank_map.setdefault(doc_id, 0.0)
        rank_map[doc_id] += 1.0 / (k + rank)
    return rank_map

def search_hybrid(query: str, top_k: int = 5, bm25_k: int = 30, vec_k: int = 30):
    bm25_res = search_bm25(query, top_k=bm25_k)
    vec_res = search_vector(query, top_k=vec_k)
    fused = rrf_fusion(bm25_res, vec_res)
    # æ’åºå–å‰ top_k
    ranked = sorted(fused.items(), key=lambda x: x[1], reverse=True)[:top_k]
    # è¿”å› documents åˆ—è¡¨
    docs = [id_to_doc[doc_id] for doc_id, _ in ranked if doc_id in id_to_doc]
    debug = {
        "bm25_top_ids": [d for d, _ in bm25_res[:10]],
        "vec_top_ids": [d for d, _ in vec_res[:10]],
        "fused_top_ids": [d for d, _ in ranked],
    }
    return docs, debug

# æ£€ç´¢ç›¸å…³æ–‡æ¡£
def search_similar(query, top_k=3):
    """æ··åˆæ£€ç´¢ï¼šBM25 + å‘é‡ + RRF"""
    try:
        docs, debug = search_hybrid(query, top_k=top_k)
        print(f"\nğŸ” Hybrid è°ƒè¯•: \n   BM25å‰10: {debug['bm25_top_ids']}\n   å‘é‡å‰10: {debug['vec_top_ids']}\n   èåˆTop: {debug['fused_top_ids']}")
        return {"documents": [docs]}
    except Exception as e:
        print(f"æ£€ç´¢å¤±è´¥(Hybrid): {e}")
        return None

# å®šä¹‰æ£€ç´¢å‡½æ•°
def retrieve_documents(query, top_k=3):
    """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
    retrieved_docs = search_similar(query, top_k=top_k)
    if retrieved_docs and retrieved_docs["documents"]:
        return retrieved_docs["documents"][0]
    return []

# ç®€å•çš„RAGç”Ÿæˆå‡½æ•°ï¼ˆæ— éœ€å¤§æ¨¡å‹APIï¼‰
def rag_retrieve_and_summarize(query):
    """æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶æä¾›ç®€å•æ€»ç»“"""
    retrieved_docs = retrieve_documents(query, top_k=3)
    
    print(f"\nğŸ” æŸ¥è¯¢: {query}")
    print("ğŸ“„ æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£:")
    for i, doc in enumerate(retrieved_docs):
        print(f"   {i+1}. {doc}")
    
    # ç®€å•çš„åŸºäºè§„åˆ™çš„æ€»ç»“
    context = "\n".join(retrieved_docs)
    
    # è®¡ç®—æŸ¥è¯¢ä¸æ–‡æ¡£çš„ç›¸å…³æ€§å¾—åˆ†ï¼ˆç®€å•å®ç°ï¼‰
    if retrieved_docs:
        print(f"\nğŸ“Š åŸºäºæ£€ç´¢çš„ä¿¡æ¯æ€»ç»“:")
        print(f"   ç›¸å…³æ–‡æ¡£æ•°é‡: {len(retrieved_docs)}")
        print(f"   ä¸»è¦å†…å®¹å…³é”®è¯: {extract_keywords(context)}")
        print(f"\nğŸ’¡ å»ºè®®ç­”æ¡ˆåŸºäºä»¥ä¸‹å†…å®¹:")
        print(f"   {context[:200]}..." if len(context) > 200 else context)
    
    return {
        "query": query,
        "retrieved_docs": retrieved_docs,
        "context": context,
        "keywords": extract_keywords(context) if retrieved_docs else []
    }

def extract_keywords(text):
    """ç®€å•çš„å…³é”®è¯æå–"""
    import re
    # ç®€å•çš„ä¸­è‹±æ–‡å…³é”®è¯æå–
    keywords = []
    # ä¸­æ–‡è¯æ±‡
    chinese_words = re.findall(r'[\u4e00-\u9fff]+', text)
    keywords.extend([w for w in chinese_words if len(w) >= 2])
    
    # è‹±æ–‡è¯æ±‡
    english_words = re.findall(r'\b[A-Za-z]{3,}\b', text)
    keywords.extend(english_words)
    
    # å»é‡å¹¶å–å‰5ä¸ª
    return list(set(keywords))[:5]


# å¯¹å¤–æš´éœ²çš„æ¥å£ï¼šæ ¹æ®æŸ¥è¯¢ç”Ÿæˆç»™å¤§æ¨¡å‹çš„æç¤ºè¯
def get_rag_prompt(query: str, top_k: int = 3) -> str:
    """
    å¤–éƒ¨è°ƒç”¨å…¥å£ï¼š
    - è¾“å…¥ query
    - å…ˆé€šè¿‡ RAG æ£€ç´¢ä¸æ±‡æ€»
    - è¿”å›ç»™å¤§æ¨¡å‹çš„ä¸­æ–‡æç¤ºè¯ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡èµ„æ–™ï¼‰
    """
    result = rag_retrieve_and_summarize(query)
    context = result.get("context", "") if isinstance(result, dict) else ""
    prompt = (
        "è¯·åŸºäºä»¥ä¸‹èµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœèµ„æ–™ä¸­æœªåŒ…å«ç­”æ¡ˆï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚\n"
        f"ç”¨æˆ·é—®é¢˜ï¼š{query}\n"
        "èµ„æ–™ï¼ˆå¯èƒ½ä¸ºå¤šæ®µï¼ŒæŒ‰ç›¸å…³æ€§æ’åºï¼‰ï¼š\n"
        f"{context}\n\n"
        "å›ç­”è¦æ±‚ï¼š\n"
        "- ä»…ä¾æ®èµ„æ–™ä½œç­”ï¼Œä¸è¦ç¼–é€ \n"
        "- å¦‚æœèµ„æ–™ä¸è¶³ä»¥å›ç­”ï¼Œè¯·è¯´æ˜\n"
        "- ä¸­æ–‡ä½œç­”ï¼Œç»™å‡ºç®€æ´ä¸”å‡†ç¡®çš„ç­”æ¡ˆ\n"
    )
    return prompt

# ä¸»ç¨‹åº
if __name__ == "__main__":

    
    #  æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    print(f"\nğŸ“Š ç³»ç»Ÿä¿¡æ¯:")
    print(f"   å‘é‡æ•°æ®åº“: ChromaDB")
    print(f"   åµŒå…¥æ¨¡å‹: {embedding_model}")
    print(f"   è‹±é›„æ–‡æ¡£æ•°é‡: {len(documents)}")
    print(f"   å­˜å‚¨ä½ç½®: ./chroma_db_local")
    print(f"   è¿è¡Œæ¨¡å¼: å®Œå…¨ç¦»çº¿")
    print(f"   æ•°æ®æ¥æº: Heroesæ–‡ä»¶å¤¹")
    
    # ç®€å•çš„å•æ¬¡æŸ¥è¯¢æ¼”ç¤º
    demo_query = "é£è¡Œè€…"
    print(f"\nğŸ¯ æ¼”ç¤ºæŸ¥è¯¢: {demo_query}")
    result = rag_retrieve_and_summarize(demo_query)
    print(result)
