# coding:utf8

import torch
import torch.nn as nn
import numpy as np
import math
import random
import os
import re
from collections import defaultdict

# --- BPE Helper Functions ---

def get_stats(ids):
    """
    è®¡ç®—idsä¸­ç›¸é‚»è¯å¯¹çš„é¢‘ç‡
    """
    counts = defaultdict(int)
    for pair in zip(ids, ids[1:]):
        counts[pair] += 1
    return counts

def merge(ids, pair, idx):
    """
    å°†idsä¸­çš„æ‰€æœ‰æŒ‡å®šè¯å¯¹æ›¿æ¢ä¸ºæ–°çš„idx
    """
    newids = []
    i = 0
    while i < len(ids):
        if i + 1 < len(ids) and (ids[i], ids[i+1]) == pair:
            newids.append(idx)
            i += 2
        else:
            newids.append(ids[i])
            i += 1
    return newids

# --- BPE Training Function ---

def train_bpe(text_data, vocab_size):
    """
    è®­ç»ƒBPEåˆ†è¯å™¨
    text_data: è®­ç»ƒè¯­æ–™ï¼Œå­—ç¬¦ä¸²
    vocab_size: ç›®æ ‡è¯è¡¨å¤§å°
    """
    # åˆå§‹è¯è¡¨ï¼šæ‰€æœ‰å¯èƒ½çš„å­—èŠ‚ (0-255)
    vocab = {idx: bytes([idx]) for idx in range(256)}
    
    # å­˜å‚¨åˆå¹¶è§„åˆ™
    merges = {}
    
    # å°†æ–‡æœ¬è½¬æ¢ä¸ºåˆå§‹å­—èŠ‚åºåˆ—
    # å¯¹äºå¤šè¡Œæ–‡æœ¬ï¼Œå°†å…¶è¿æ¥èµ·æ¥ï¼Œç„¶åç¼–ç 
    byte_tokens = list(text_data.encode("utf-8"))
    
    # è¿›è¡Œåˆå¹¶æ“ä½œ
    for i in range(256, vocab_size):
        # è®¡ç®—å½“å‰tokenåºåˆ—ä¸­æ‰€æœ‰è¯å¯¹çš„é¢‘ç‡
        stats = get_stats(byte_tokens)
        
        if not stats:
            break # æ²¡æœ‰å¯ä»¥åˆå¹¶çš„è¯å¯¹
            
        # æ‰¾åˆ°é¢‘ç‡æœ€é«˜çš„è¯å¯¹
        best_pair = max(stats, key=stats.get)
        
        # å°†è¯¥è¯å¯¹æ·»åŠ åˆ°åˆå¹¶è§„åˆ™ä¸­ï¼Œå¹¶åˆ†é…æ–°çš„token ID
        merges[best_pair] = i
        vocab[i] = vocab[best_pair[0]] + vocab[best_pair[1]]
        
        # å°†æ‰€æœ‰å‡ºç°è¯¥è¯å¯¹çš„åœ°æ–¹è¿›è¡Œåˆå¹¶
        byte_tokens = merge(byte_tokens, best_pair, i)
        
        print(f"åˆå¹¶ {best_pair} -> {i} ({vocab[i].decode('utf-8', errors='ignore')}), è¯è¡¨å¤§å°: {len(vocab)}")
    
    return merges, vocab

# --- Encode and Decode Functions ---

def encode(text, merges, vocab):
    """
    ç»™å®šä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä½¿ç”¨BPEè§„åˆ™å°†å…¶ç¼–ç ä¸ºtoken IDåˆ—è¡¨
    """
    # å°†æ–‡æœ¬è½¬æ¢ä¸ºåˆå§‹å­—èŠ‚åºåˆ—
    tokens = list(text.encode("utf-8"))
    
    # è¿­ä»£åº”ç”¨åˆå¹¶è§„åˆ™ï¼Œç›´åˆ°æ²¡æœ‰æ›´å¤šå¯ä»¥åˆå¹¶çš„è¯å¯¹
    while True:
        stats = get_stats(tokens)
        
        # æ‰¾åˆ°å½“å‰tokenåºåˆ—ä¸­æœ€ä¼˜ï¼ˆåœ¨mergesä¸­ï¼‰çš„è¯å¯¹
        # key=lambda p: merges.get(p, float("inf")) è¡¨ç¤ºå¦‚æœpairä¸åœ¨mergesä¸­ï¼Œåˆ™ä¼˜å…ˆçº§æœ€ä½
        pair = min(stats, key=lambda p: merges.get(p, float("inf")))
        
        # å¦‚æœæœ€ä¼˜å…ˆçš„è¯å¯¹ä¸åœ¨mergesä¸­ï¼Œè¯´æ˜æ²¡æœ‰æ›´å¤šå¯åˆå¹¶çš„äº†
        if pair not in merges:
            break
        
        idx = merges[pair]
        tokens = merge(tokens, pair, idx)
    return tokens

def decode(ids, vocab):
    """
    ç»™å®štoken IDåˆ—è¡¨ï¼Œå°†å…¶è§£ç å›Pythonå­—ç¬¦ä¸²
    """
    # å°†token IDåºåˆ—è½¬æ¢å›å­—èŠ‚åºåˆ—
    tokens = b"".join(vocab[idx] for idx in ids)
    # å°†å­—èŠ‚åºåˆ—è§£ç ä¸ºUTF-8å­—ç¬¦ä¸²
    text = tokens.decode("utf-8", errors="replace")
    return text

# --- Main Execution ---

if __name__ == "__main__":
    # --- å‡†å¤‡è®­ç»ƒæ•°æ® ---
    # ä½¿ç”¨è¾ƒé•¿çš„æ–‡æœ¬æ¥è®­ç»ƒBPEï¼Œä»¥è·å¾—æ›´æœ‰æ„ä¹‰çš„åˆå¹¶
    # è¿™é‡Œä½¿ç”¨æ‚¨ä¹‹å‰æä¾›çš„valtextä½œä¸ºè®­ç»ƒæ•°æ®
    text = "Many common characters, including numerals, punctuation, and other symbols, are unified within the standard and are not treated as specific to any given writing system. Unicode encodes thousands of emoji, with the continued development thereof conducted by the Consortium as a part of the standard.[4] Moreover, the widespread adoption of Unicode was in large part responsible for the initial popularization of emoji outside of Japan. Unicode is ultimately capable of encoding more than 1.1 million characters."
    
    # å¢åŠ ä¸€äº›åŒ…å«ç‰¹æ®Šå­—ç¬¦å’ŒéŸ©è¯­çš„æ–‡æœ¬ï¼Œç¡®ä¿BPEèƒ½å¤„ç†å¤šè¯­è¨€
    text_data = text + "ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ (hello in Korean!) " + \
                "ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception."

    # --- è®­ç»ƒBPEåˆ†è¯å™¨ ---
    print("--- å¼€å§‹è®­ç»ƒBPEåˆ†è¯å™¨ ---")
    # ç›®æ ‡è¯è¡¨å¤§å°ï¼Œä¾‹å¦‚ 500 ä¸ª token (256ä¸ªå­—èŠ‚ + 244ä¸ªåˆå¹¶)
    BPE_VOCAB_SIZE = 500 
    merges, vocab = train_bpe(text_data, BPE_VOCAB_SIZE)
    print("--- BPEåˆ†è¯å™¨è®­ç»ƒå®Œæˆ ---")
    print(f"æœ€ç»ˆè¯è¡¨å¤§å°: {len(vocab)}")
    print(f"åˆå¹¶è§„åˆ™æ•°é‡: {len(merges)}")
    print("\néƒ¨åˆ†åˆå¹¶è§„åˆ™:")
    for (pair, new_id) in list(merges.items())[:10]:
        print(f"  {pair} -> {new_id} ({vocab[new_id].decode('utf-8', errors='ignore')})")


    # --- æµ‹è¯• encode å’Œ decode åŠŸèƒ½ ---
    print("\n--- æµ‹è¯•ç¼–ç å’Œè§£ç  ---")

    test_string_1 = "A Programmerâ€™s Introduction to Unicode"
    encoded_ids_1 = encode(test_string_1, merges, vocab)
    decoded_text_1 = decode(encoded_ids_1, vocab)
    print(f"åŸå§‹æ–‡æœ¬: '{test_string_1}'")
    print(f"ç¼–ç IDs: {encoded_ids_1}")
    print(f"è§£ç æ–‡æœ¬: '{decoded_text_1}'")
    print(f"è§£ç æ˜¯å¦åŒ¹é…åŸå§‹æ–‡æœ¬: {decoded_text_1 == test_string_1}")
    
    print("-" * 30)

    test_string_2 = "ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ (hello in Korean!)"
    encoded_ids_2 = encode(test_string_2, merges, vocab)
    decoded_text_2 = decode(encoded_ids_2, vocab)
    print(f"åŸå§‹æ–‡æœ¬: '{test_string_2}'")
    print(f"ç¼–ç IDs: {encoded_ids_2}")
    print(f"è§£ç æ–‡æœ¬: '{decoded_text_2}'")
    print(f"è§£ç æ˜¯å¦åŒ¹é…åŸå§‹æ–‡æœ¬: {decoded_text_2 == test_string_2}")

    print("-" * 30)

    valtext = "Many common characters, including numerals, punctuation, and other symbols, are unified within the standard and are not treated as specific to any given writing system. Unicode encodes thousands of emoji, with the continued development thereof conducted by the Consortium as a part of the standard.[4] Moreover, the widespread adoption of Unicode was in large part responsible for the initial popularization of emoji outside of Japan. Unicode is ultimately capable of encoding more than 1.1 million characters."
    valtext2 = decode(encode(valtext, merges, vocab), vocab)
    print(f"é•¿æ–‡æœ¬è§£ç æ˜¯å¦åŒ¹é…åŸå§‹æ–‡æœ¬: {valtext2 == valtext}")

    print("-" * 30)
    # æµ‹è¯•ä¸€äº›æœªåœ¨è®­ç»ƒè¯­æ–™ä¸­ä½†BPEåº”è¯¥èƒ½å¤„ç†çš„æ–‡æœ¬
    test_string_3 = "è¿™æ˜¯ä¸€ä¸ªæ–°çš„ä¸­æ–‡å¥å­ã€‚"
    encoded_ids_3 = encode(test_string_3, merges, vocab)
    decoded_text_3 = decode(encoded_ids_3, vocab)
    print(f"åŸå§‹æ–‡æœ¬: '{test_string_3}'")
    print(f"ç¼–ç IDs: {encoded_ids_3}")
    print(f"è§£ç æ–‡æœ¬: '{decoded_text_3}'")
    print(f"è§£ç æ˜¯å¦åŒ¹é…åŸå§‹æ–‡æœ¬: {decoded_text_3 == test_string_3}")

    print("\n--- ç¼–ç åŸå§‹æ–‡æœ¬å¹¶è®¡ç®—å‹ç¼©ç‡ ---")
    original_bytes_len = len(text_data.encode("utf-8"))
    encoded_tokens = encode(text_data, merges, vocab)
    encoded_ids_len = len(encoded_tokens)
    print(f"åŸå§‹å­—èŠ‚é•¿åº¦: {original_bytes_len}")
    print(f"ç¼–ç Token IDé•¿åº¦: {encoded_ids_len}")
    print(f"å‹ç¼©ç‡ (å­—èŠ‚/Token ID): {original_bytes_len / encoded_ids_len:.2f}X")
