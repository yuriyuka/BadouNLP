import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from torch.nn.utils.rnn import pad_sequence

# å‚æ•°è®¾ç½®
vocab_size = 27  # a-zåŠ ä¸Šç‰¹æ®Šå­—ç¬¦
max_length = 20  # å­—ç¬¦ä¸²æœ€å¤§é•¿åº¦
samples = 10000  # æ ·æœ¬æ•°é‡
batch_size = 64  # æ‰¹æ¬¡å¤§å°
embedding_dim = 16  # åµŒå…¥ç»´åº¦
hidden_dim = 32  # éšè—å±‚ç»´åº¦
epochs = 20  # è®­ç»ƒè½®æ¬¡

# åºåˆ—å¡«å……å‡½æ•°ï¼ˆå…¨å±€å®šä¹‰ï¼‰
def collate_fn(batch):
    """
    å¯¹æ‰¹æ¬¡æ•°æ®è¿›è¡Œå¡«å……å’Œæˆªæ–­å¤„ç†

    å‚æ•°:
        batch: åŒ…å«å¤šä¸ªæ ·æœ¬çš„åˆ—è¡¨ï¼Œæ¯ä¸ªæ ·æœ¬ä¸º(åºåˆ—å¼ é‡, æ ‡ç­¾)å…ƒç»„

    è¿”å›:
        tuple: å¡«å……åçš„åºåˆ—å¼ é‡å’Œå †å åçš„æ ‡ç­¾å¼ é‡
    """
    sequences = [item[0] for item in batch]
    labels = [item[1] for item in batch]
    padded = pad_sequence(sequences, batch_first=True, padding_value=0)
    if padded.size(1) > max_length:
        padded = padded[:, :max_length]
    return padded, torch.stack(labels)

# ç”Ÿæˆæ•°æ®
def generate_data(samples, max_length):
    """
    ç”ŸæˆåŒ…å«éšæœºå­—ç¬¦ä¸”åŒ…å«ç‰¹å®šæ ‡è®°å­—ç¬¦'a'çš„åºåˆ—æ•°æ®é›†

    å‚æ•°:
        samples: éœ€è¦ç”Ÿæˆçš„æ ·æœ¬æ€»æ•°
        max_length: åºåˆ—çš„æœ€å¤§å…è®¸é•¿åº¦

    è¿”å›:
        tuple: åŒ…å«ç‰¹å¾åºåˆ—(X)å’Œç›®æ ‡ä½ç½®(y)çš„å…ƒç»„
    """
    X = []
    y = []

    for _ in range(samples):
        length = np.random.randint(1, max_length + 1)
        a_position = np.random.randint(0, length)

        chars = []
        for i in range(length):
            if i == a_position:
                chars.append('a')
            else:
                char_code = np.random.randint(98, 123)
                chars.append(chr(char_code))

        seq = [ord(c) - 96 for c in chars]  # å°†å­—ç¬¦è½¬æ¢ä¸ºæ•°å€¼ç´¢å¼•
        X.append(seq)
        y.append(a_position)

    return X, y

# åˆ›å»ºæ•°æ®é›†ç±»
class TextDataset(Dataset):
    """
    è‡ªå®šä¹‰æ•°æ®é›†ç±»ï¼Œç”¨äºåŠ è½½åºåˆ—æ•°æ®å’Œå¯¹åº”æ ‡ç­¾

    å±æ€§:
        sequences: å­˜å‚¨ç‰¹å¾åºåˆ—çš„åˆ—è¡¨
        labels: å­˜å‚¨ç›®æ ‡ä½ç½®çš„å¼ é‡
        max_length: åºåˆ—æœ€å¤§é•¿åº¦é™åˆ¶
    """
    def __init__(self, sequences, labels, max_length):
        self.sequences = sequences
        self.labels = torch.tensor(labels, dtype=torch.long)
        self.max_length = max_length

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        return torch.tensor(self.sequences[idx], dtype=torch.long), self.labels[idx]

# å®šä¹‰RNNæ¨¡å‹
class RNNClassifier(nn.Module):
    """
    åŸºäºRNNçš„åˆ†ç±»æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹ç‰¹å®šå­—ç¬¦é¦–æ¬¡å‡ºç°çš„ä½ç½®

    å±æ€§:
        embedding: åµŒå…¥å±‚ï¼Œå°†ç¦»æ•£å­—ç¬¦ç´¢å¼•æ˜ å°„ä¸ºè¿ç»­å‘é‡
        rnn: RNNå±‚ï¼Œå¤„ç†åºåˆ—æ•°æ®
        fc: å…¨è¿æ¥å±‚ï¼Œè¾“å‡ºåˆ†ç±»ç»“æœ
    """
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(RNNClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­è¿‡ç¨‹

        å‚æ•°:
            x: è¾“å…¥åºåˆ—å¼ é‡ï¼Œå½¢çŠ¶ä¸º(batch_size, sequence_length)

        è¿”å›:
            tensor: è¾“å‡ºé¢„æµ‹ç»“æœï¼Œå½¢çŠ¶ä¸º(batch_size, output_dim)
        """
        embedded = self.embedding(x)
        _, hidden = self.rnn(embedded)
        output = self.fc(hidden.squeeze(0))
        return output

# å°è£…è®­ç»ƒé€»è¾‘
def train_epoch(model, loader, optimizer, criterion, device):
    """
    æ‰§è¡Œå•ä¸ªè®­ç»ƒå‘¨æœŸçš„å®Œæ•´æµç¨‹

    å‚æ•°:
        model: å¾…è®­ç»ƒçš„ç¥ç»ç½‘ç»œæ¨¡å‹
        loader: æ•°æ®åŠ è½½å™¨
        optimizer: ä¼˜åŒ–å™¨
        criterion: æŸå¤±å‡½æ•°
        device: è®¡ç®—è®¾å¤‡(CPU/GPU)

    è¿”å›:
        tuple: å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡ç™¾åˆ†æ¯”
    """
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for inputs, targets in loader:
        inputs, targets = inputs.to(device), targets.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

    return total_loss / len(loader), 100. * correct / total

# å°è£…éªŒè¯é€»è¾‘
def evaluate(model, loader, criterion, device):
    """
    è¯„ä¼°æ¨¡å‹åœ¨éªŒè¯/æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½

    å‚æ•°:
        model: å¾…è¯„ä¼°çš„ç¥ç»ç½‘ç»œæ¨¡å‹
        loader: æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        device: è®¡ç®—è®¾å¤‡(CPU/GPU)

    è¿”å›:
        tuple: å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡ç™¾åˆ†æ¯”
    """
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, targets in loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)

            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

    return total_loss / len(loader), 100. * correct / total

# é¢„æµ‹ç¤ºä¾‹
def predict_example(model, sample, true_label, device):
    """
    å±•ç¤ºæ¨¡å‹å¯¹å•ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ

    å‚æ•°:
        model: è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œæ¨¡å‹
        sample: å•ä¸ªç‰¹å¾åºåˆ—æ ·æœ¬
        true_label: çœŸå®çš„ç›®æ ‡ä½ç½®æ ‡ç­¾
        device: è®¡ç®—è®¾å¤‡(CPU/GPU)
    """
    model.eval()
    # åŠ è½½æœ€ä¼˜æ¨¡å‹æƒé‡
    model.load_state_dict(torch.load(best_model_path))
    print("ğŸ” å·²åŠ è½½æœ€ä¼˜æ¨¡å‹æƒé‡")
    with torch.no_grad():
        # âœ… åˆ›å»ºå¼ é‡åç«‹å³ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        sample_tensor = torch.tensor(sample, dtype=torch.long).to(device)

        # å¡«å……/æˆªæ–­é€»è¾‘
        if len(sample_tensor) > max_length:
            sample_tensor = sample_tensor[:max_length]
        else:
            padding = torch.zeros(max_length - len(sample_tensor), dtype=torch.long).to(device)
            sample_tensor = torch.cat((sample_tensor, padding))

        # æ·»åŠ batchç»´åº¦
        sample_tensor = sample_tensor.unsqueeze(0)

        # æ¨¡å‹é¢„æµ‹
        output = model(sample_tensor)
        probs = torch.softmax(output, dim=1)
        predicted_label = torch.argmax(probs, dim=1).item()

        # è½¬æ¢å›å­—ç¬¦ä¸²è¡¨ç¤º
        chars = []
        for idx in sample_tensor.squeeze(0).cpu().numpy()[:len(sample)]:
            if idx == 0:
                chars.append('-')
            else:
                chars.append(chr(idx + 96))
        string = ''.join(chars)

        print(f"ç¤ºä¾‹å­—ç¬¦ä¸²: {string}")
        print(f"çœŸå®é¦–æ¬¡å‡ºç°'a'çš„ä½ç½®: {true_label}")
        print(f"é¢„æµ‹é¦–æ¬¡å‡ºç°'a'çš„ä½ç½®: {predicted_label}")
        print(f"é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ: {probs.cpu().numpy().round(3)}")

if __name__ == '__main__':
    # è®¾ç½®GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    best_val_acc = 0.0
    best_model_path = 'best_model.pth'

    # ç”Ÿæˆæ•°æ®
    X, y = generate_data(samples, max_length)

    # åˆ†å‰²æ•°æ®é›†
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.33, random_state=42)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TextDataset(X_train, y_train, max_length)
    val_dataset = TextDataset(X_val, y_val, max_length)
    test_dataset = TextDataset(X_test, y_test, max_length)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, collate_fn=collate_fn)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4, collate_fn=collate_fn)

    # åˆå§‹åŒ–æ¨¡å‹
    model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, max_length).to(device)

    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters())

    # è®­ç»ƒæ¨¡å‹
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []

    # for epoch in range(epochs):
    #     train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)
    #     val_loss, val_acc = evaluate(model, val_loader, criterion, device)
    #
    #     train_losses.append(train_loss)
    #     val_losses.append(val_loss)
    #     train_accs.append(train_acc)
    #     val_accs.append(val_acc)
    #
    #     print(
    #         f'Epoch {epoch + 1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')
    #     # ä¿å­˜æœ€ä¼˜æ¨¡å‹
    #     if val_acc > best_val_acc:
    #         best_val_acc = val_acc
    #         torch.save(model.state_dict(), best_model_path)
    #         print(f"âœ… ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
    # # è¯„ä¼°æ¨¡å‹
    # test_loss, test_acc = evaluate(model, test_loader, criterion, device)
    # print(f'æµ‹è¯•é›†ç»“æœ: Loss: {test_loss:.4f}, Acc: {test_acc:.2f}%')
    #
    # # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
    # plt.figure(figsize=(12, 4))
    # plt.subplot(1, 2, 1)
    # plt.plot(train_losses, label='è®­ç»ƒæŸå¤±')
    # plt.plot(val_losses, label='éªŒè¯æŸå¤±')
    # plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
    # plt.xlabel('è½®æ¬¡')
    # plt.ylabel('æŸå¤±')
    # plt.legend()
    #
    # plt.subplot(1, 2, 2)
    # plt.plot(train_accs, label='è®­ç»ƒå‡†ç¡®ç‡')
    # plt.plot(val_accs, label='éªŒè¯å‡†ç¡®ç‡')
    # plt.title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡')
    # plt.xlabel('è½®æ¬¡')
    # plt.ylabel('å‡†ç¡®ç‡ (%)')
    # plt.legend()
    # plt.tight_layout()
    # plt.show()




    # æ˜¾ç¤ºå‡ ä¸ªé¢„æµ‹ç¤ºä¾‹
    for i in range(3):
        print(f"\n=== ç¤ºä¾‹ {i + 1} ===")
        predict_example(model, X_test[i], y_test[i], device)
